{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Notes","text":"<p>Welcome to my research notes and documentation. This site contains a collection of notes, guides, and references covering various topics in machine learning, audio processing, Python programming, and software development tools.</p>"},{"location":"#whats-inside","title":"What's Inside","text":""},{"location":"#machine-learning","title":"\ud83e\udd16 Machine Learning","text":"<p>Concepts, algorithms, and implementations covering: - Neural networks and deep learning architectures - Mathematical foundations (softmax, k-means, GMM, probability) - Research notes and practical applications</p>"},{"location":"#audio-processing","title":"\ud83d\udd0a Audio Processing","text":"<p>Notes on speech and audio technologies: - Speech enhancement metrics (PESQ, STOI, etc.) - Audio processing techniques - Deep learning for audio</p>"},{"location":"#papers","title":"\ud83d\udcc4 Papers","text":"<p>Summaries and notes on research papers: - HuBERT - Predictive speech representation learning - wav2vec 2.0 - Self-supervised speech representations - Sidon - Speech understanding - Reading list of papers to explore</p>"},{"location":"#python","title":"\ud83d\udc0d Python","text":"<p>Python programming guides and tips: - Debugger usage - Path handling - Unpack operators - Best practices</p>"},{"location":"#guides","title":"\ud83d\udee0\ufe0f Guides","text":"<p>Practical guides for development tools: - Version control (Git) - Terminal multiplexers (tmux, screen) - Editors (vim) - Package managers (conda, uv) - Configuration management (Hydra) - SSH and remote access - Shell tips and tricks</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Questions - Common questions and answers</li> <li>Acronyms - Reference list of acronyms and abbreviations</li> <li>Tools Guide - Overview of development tools</li> </ul>"},{"location":"#about","title":"About","text":"<p>These notes are continuously updated as I learn and explore new topics. They serve as a personal reference and knowledge base.</p>"},{"location":"acronyms/","title":"A nice list of acronyms","text":"<ul> <li> <p>LoRA: Low Rank Adaptation</p> </li> <li> <p>SSL: SelfSupervised Learning</p> </li> <li> <p>TTS: Text To Speech</p> </li> <li> <p>VAD: Voice Activity Detection</p> </li> </ul>"},{"location":"notes/","title":"Random notes","text":""},{"location":"notes/#ctc-loss","title":"CTC Loss","text":"<p>Connectionist Temporal Classification it is a loss used for RNNs, seems to be the go to for phonemes classification. Basically ignores repetition and blanks in sequences, since in audio there are much more frames than phonemes.</p> <p><code>C C A - - T -</code> \u2192 <code>C A T</code> </p>"},{"location":"questions/","title":"Questions that will need an answer","text":""},{"location":"questions/#how-does-pickle-work-what-makes-something-pickleable","title":"How does pickle work? What makes something pickleable?","text":"<p>Well basically it takes an object, serialize it and save it to a compressed binary. Something is not pickleable when it is not serializable. The problem that we were experiencing was related to that:</p> <pre><code>class A:\n\n    def __init__(self):\n        self.pointer = cpointer() # lets say that this is some c pointer, an address, not a value\n\na = A()\n\npickle.dump(A) # works, class itself contains the constructor, not the pointer\npickle.dump(a) # fails, instance contains the pointer\n</code></pre> <p>So what have we learned? If possible minimize pointers and make them as local as possible</p>"},{"location":"questions/#how-does-onnx-work-what-makes-something-onnx-compatible","title":"How does ONNX work? What makes something ONNX compatible?","text":""},{"location":"questions/#how-do-dataclasses-work-everything-about-dataclass-property-field","title":"How do dataclasses work? Everything about dataclass, property, field","text":""},{"location":"questions/#how-to-use-the-python-debugger-from-the-terminal","title":"How to use the python debugger from the terminal","text":""},{"location":"questions/#how-to-use-a-profiler-preferably-from-the-terminal","title":"How to use a profiler preferably from the terminal","text":""},{"location":"questions/#how-does-python-test-ing-actually-workhttpswwwyoutubecomwatchv6lvu6wpurs0","title":"How does python test ing actually work?https://www.youtube.com/watch?v=6Lvu6WPuRS0","text":""},{"location":"questions/#what-is-nose2-why-would-one-use-that-to-test-things-instead-of-something-else","title":"What is nose2? why would one use that to test things instead of something else?","text":""},{"location":"tools-guide/","title":"Guide of things that i intend to use","text":""},{"location":"tools-guide/#tmux","title":"Tmux","text":"<ul> <li><code>C</code> means Ctrl</li> <li><code>C-b</code> is the prefix for everything</li> <li><code>C-b ?</code> is the manual</li> <li><code>C-b %</code> splits vertically</li> <li><code>C-b arrows</code> to navigate the panes</li> </ul> <p>There are probably much smarter things that one can do</p> <p>Okay but what are this panes windows session, thats a bit hard to understand</p>"},{"location":"tools-guide/#sessions-windows-and-panes","title":"Sessions, windows and panes","text":"<p>A Session is the big container, with inside windows, and inside those panes, when you split the screen you are creating new panes.</p> <ul> <li><code>tmux a</code> to attach to a session from outside, the only session or the last one</li> <li><code>tmux a -t &lt;session_name&gt;</code> to a ttach to a specific session</li> <li><code>exit</code> to exit and kill the session</li> <li><code>C-b d</code> to detach from the session and let it run</li> </ul>"},{"location":"audio/","title":"Audio","text":"<p>Notes on audio processing, speech enhancement, and related topics.</p>"},{"location":"audio/short-time-intelligibility/","title":"Short Time Objective Intelligibility","text":"<p>STOI (2011) is an intrusive metric, meaning that it requires a clean reference and it measures degradation.</p> <p><code>torchmetrics.audio.stoi</code>Short-Time Objective Intelligibility (STOI) \u2014 PyTorch-Metrics 1.8.2 documentation</p> <p><code>pystoy</code> GitHub - mpariente/pystoi: Python implementation of the Short Term Objective Intelligibility measure</p>"},{"location":"audio/short-time-intelligibility/#steps","title":"Steps","text":"<ul> <li> <p>downsampling to 10khz</p> </li> <li> <p>splitting into frames (256 samples), 50% overlap, hanning window</p> </li> <li> <p>FFT</p> </li> <li> <p>Grouping into 15 1/3 octave bands</p> </li> <li> <p>extract band energy</p> </li> <li> <p>create envelope matrix, looking at the 30 previous frames, at this point we have a matrix E (15 \\times 30), with an envelope for each band</p> </li> <li> <p>normalization, each row is normalized so the total energy in a specific row is the same in the clean and processed signal</p> </li> <li> <p>clipping</p> </li> <li> <p>per-row correlation: Pearson correlation is calculated between each row clean and processed</p> </li> <li> <p>frame correlation: the 15 correlations are averaged and we get the STOI for that fragment</p> </li> <li> <p>global correlation: the entire process is repeated sliing by one frame and then the global average is calculated giving us the final STOI s \\in [0,1], with a good value being s \\ge 0.85 and a bad one being s \\le 0.60.</p> </li> </ul>"},{"location":"audio/short-time-intelligibility/#pros-and-cons","title":"Pros and Cons","text":"<p>Pros:</p> <ul> <li>Perceptually informed</li> <li>non differentiable, due to clipping, normalization and also hard filters create a very jagged gradient (it can be fixed tho, with soft clipping and epsilon padding)</li> </ul> <p>Cons:</p> <ul> <li> <p>Contrastive, we need a clean signal</p> </li> <li> <p>Disregard phase, so something like a vocoder still works while sounding completely unnatural, but this is by design, we're measuring intelligibility, not quality</p> </li> <li> <p>Normalization step can be problematic if a model attenuates or amplifies specific bands to the point of unintelligibility</p> </li> </ul>"},{"location":"audio/short-time-intelligibility/#local-stoi-and-stoi-gram","title":"Local STOI and STOI-gram","text":""},{"location":"audio/short-time-intelligibility/#stoi-net","title":"STOI-Net","text":""},{"location":"audio/short-time-intelligibility/#extended-stoi","title":"extended STOI","text":""},{"location":"audio/short-time-intelligibility/#usage","title":"Usage","text":""},{"location":"audio/speech-enhancement-metrics/","title":"Metrics and Losses","text":""},{"location":"audio/speech-enhancement-metrics/#what-are-we-doing","title":"What are we doing?","text":"<p>We are trying to build an algorithm, \\(f\\), that given a suggestion will try to make  a guess, and we want to measure how close it got to the truth in order to make the algorithm better.</p>"},{"location":"audio/speech-enhancement-metrics/#what-why","title":"What? Why?","text":""},{"location":"audio/speech-enhancement-metrics/#mmmh-okay-go-ahead","title":"Mmmh, okay go ahead...","text":"<p>We have four simple blocks:</p> <ul> <li> <p>\\(y\\): the truth, or that we don't know and we're trying to guess</p> </li> <li> <p>\\(x\\): our input</p> </li> <li> <p>\\(f\\): our algorithm, or model, to get close from to the truth with our input</p> </li> <li> <p>\\(\\theta\\): additional parameters for our algorithm, independent from the input</p> </li> </ul> <p>With those blocks we can build two key things:</p> <ul> <li> <p>\\(\\hat y = f(x, \\theta)\\) our guess, or prediction with the available input</p> </li> <li> <p>\\(y - \\hat y\\): the distance between our guess and the truth, called residual</p> </li> </ul> <p>There is also an important thing to mention, we are being very general here and because of that we need to understand how </p>"},{"location":"audio/speech-enhancement-metrics/#intuition-for-those-formulas","title":"Intuition for those formulas","text":"<p>Stuff like \\(\\frac{1}{N}\\) and \\(\\sum_{i=1}^N\\) doesn't really matter, this is just means the average. the most another important thing is that \\(y_i-f(x_i)\\) is just the distance from the truth, that's the input to our actual measurament. The point is that \\(y_i\\) can be any number, and we are interested in reaching that number so we measure how far we are from it and then we calculcate our gradient. \\(y_i - f(x_i)\\) is just placing ourself on the correct spot on the x axis, before appling the function whose slope we're interested about. </p>"},{"location":"audio/speech-enhancement-metrics/#mean-bias-error","title":"Mean Bias Error","text":"<p>Mean bias error (MBE): this is kinda of a dumb one, is just the difference sample by sample, not very useful since some are positive and some are negative and the loss can go to \\(0\\) even if the samples are very different.</p> <p>On the good side it is differentiable. </p> \\[ L_{MBE}=\\frac{1}{N}\\sum_{i=1}^N (y_i-f(x_i)) \\]"},{"location":"audio/speech-enhancement-metrics/#mean-absolute-error","title":"Mean Absolute Error","text":"<p>Mean Absolute Error (MAE), we now take the absolutte difference sample by sample, which makes much more sense but it has the problem that it is not differentiable, given that \\(\\nexists f'(|x|)\\) when \\(x = 0\\). Besides that it is quite good to compare signals because it is equally sensitive to small and big differences. Which is not the case with squared or other.</p> \\[ L_{MAE} = \\frac{1}{N}\\sum_{i=1}^N |y_i -f(x_i)| \\] <p>Anyway, in practice, when we match ground truth and theoretically cannot differentiate we can just pick either \\(1\\) or \\(-1\\) as a gradient and pretend to be off by an \\(\\epsilon\\). </p> <p>This thing is also called L1 (<code>torch.nn.L1Loss</code>) because it is  based on the \\(L_1\\) norm of a vector, which is the sum of the magnitudes of its components. The distance \\(\\sum|v-w|\\) is also called manhattan or taxicab distance, because we're summing the absolute difference on each axis, so it's like travelling on a grid. </p> <p>Even another term is Least Absolute Deviations (LAD). I honestly don't care why about this one, we already have enough different names.</p>"},{"location":"audio/speech-enhancement-metrics/#mean-squared-error","title":"Mean Squared Error","text":"\\[ L_{MSE} = \\frac{1}{N}\\sum_{i=1}^N(y_i-f(x_i))^2 \\] <ul> <li> <p>\\(L_2\\) </p> </li> <li> <p><code>torch.nn.L2Loss</code></p> </li> </ul>"},{"location":"audio/speech-enhancement-metrics/#snr","title":"SNR","text":""},{"location":"audio/speech-enhancement-metrics/#sdr","title":"SDR","text":""},{"location":"audio/speech-enhancement-metrics/#si-sdr","title":"SI-SDR","text":""},{"location":"audio/speech-enhancement-metrics/#si-snr","title":"SI-SNR","text":""},{"location":"audio/speech-enhancement-metrics/#perceptual-evaluation-of-speech-quality","title":"Perceptual Evaluation of Speech Quality","text":"<p>Also it is probably useless, all the models that I trained seems to have 0 STOI, so they're suppoesed to be perfect for this metric, but they're NOT GOOD ENOUGH.</p>"},{"location":"audio/speech-enhancement-metrics/#short-time-objective-intelligibility","title":"Short Time Objective Intelligibility","text":"<p>STOI (2011) is an intrusive metric, meaning that it requires a clean reference and it measures degradation.</p> <p><code>torchmetrics.audio.stoi</code>[Short-Time Objective Intelligibility (STOI) \u2014 PyTorch-Metrics 1.8.2 documentation](https://lightning.ai/docs/torchmetrics/stable/audio/short_time_objective_intelligibility.html)</p> <p><code>pystoy</code> [GitHub - mpariente/pystoi: Python implementation of the Short Term Objective Intelligibility measure](https://github.com/mpariente/pystoi)</p> <p>``</p>"},{"location":"audio/speech-enhancement-metrics/#steps","title":"Steps","text":"<ul> <li> <p>downsampling to 10khz</p> </li> <li> <p>splitting into frames (256 samples), 50% overlap, hanning window</p> </li> <li> <p>FFT</p> </li> <li> <p>Grouping into 15 1/3 octave bands</p> </li> <li> <p>extract band energy</p> </li> <li> <p>create envelope matrix, looking at the 30 previous frames, at this point we have a matrix \\(E (15 \\times 30)\\), with an envelope for each band</p> </li> <li> <p>normalization, each row is normalized so the total energy in a specific row is the same in the clean and processed signal</p> </li> <li> <p>clipping</p> </li> <li> <p>per-row correlation: Pearson correlation is calculated between each row clean and processed</p> </li> <li> <p>frame correlation: the 15 correlations are averaged and we get the STOI for that fragment</p> </li> <li> <p>global correlation: the entire process is repeated sliing by one frame and then the global average is calculated giving us the final STOI \\(s \\in [0,1]\\), with a good value being \\(s \\ge 0.85\\) and a bad one being \\(s \\le 0.60\\).</p> </li> </ul>"},{"location":"audio/speech-enhancement-metrics/#pros-and-cons","title":"Pros and Cons","text":"<p>Pros:</p> <ul> <li>Perceptually informed</li> <li>non differentiable, due to clipping, normalization and also hard filters create a very  jagged gradient</li> </ul> <p>Cons:</p> <ul> <li> <p>Contrastive, we need a clean signal</p> </li> <li> <p>Disregard phase, so something like a vocoder still works while sounding completely unnatural, but this is by design, we're measuring intelligibility, not quality</p> </li> <li> <p>Normalization step can be problematic if a model attenuates or amplifies specific bands to the point of unintelligibility</p> </li> </ul>"},{"location":"audio/speech-enhancement-metrics/#stoi-net","title":"STOI-Net","text":""},{"location":"audio/speech-enhancement-metrics/#extended-stoi","title":"extended STOI","text":""},{"location":"audio/speech-enhancement-metrics/#usage","title":"Usage","text":""},{"location":"audio/speech-enhancement-metrics/#urgent","title":"URGENT","text":""},{"location":"audio/speech-enhancement-metrics/#sigmos","title":"SIGMOS","text":""},{"location":"audio/speech-enhancement-metrics/#speech-intelligibility-index","title":"Speech Intelligibility Index","text":""},{"location":"guides/","title":"Guides","text":"<p>Collection of guides and tutorials for various tools and technologies.</p>"},{"location":"guides/conda/","title":"Conda","text":"<p>I'll be using minconda, even though the differences should be investigated. Basically it's an environment, a bit similar to venv bu much bigger, one can  also change the version of python and other software. Basically if you are in a conda environment when you call a command first the conda enviroment gets searched and afterwards the parent one (I think, maybe not even always,  probably depends on the command)</p>"},{"location":"guides/conda/#commands","title":"Commands","text":"<ul> <li><code>conda create --name new-env-cloned --clone env-to-be-cloned</code> creates an env by cloning the target one</li> <li><code>conda env create -f environment.yml</code> create an environment from .yml file</li> <li><code>conda env list</code> lists the environments and <code>*</code> on the current one</li> <li><code>conda env remove &lt;env-to-be-removed&gt;</code> deletes and environment</li> <li><code>conda rename -n &lt;current-env-name&gt; &lt;new-env-name&gt;</code></li> </ul>"},{"location":"guides/conda/#conda-vs-mamba","title":"Conda vs Mamba","text":"<p>Apparently you can also install packages in your conda environment with mamba, no idea what the difference could be</p>"},{"location":"guides/git/","title":"GIT","text":"<p>Well i don't know shit about, git, I should learn some more, it could come in very handy.</p> <ul> <li> <p><code>git clone &lt;repo-link&gt; &lt;target-folder&gt;</code> download repo from the internets</p> </li> <li> <p><code>git pull</code> download updates from the online repo</p> </li> <li> <p><code>git push</code> push updates to online repo</p> </li> </ul> <p>What does origin mean?</p> <ul> <li><code>git branch</code> see local branches</li> <li><code>git branch -r</code> see remote branches</li> <li><code>git checkout &lt;branch-name&gt;</code> switch branch</li> </ul> <p>And all the other things </p>"},{"location":"guides/glow-and-markdown/","title":"Markdown","text":"<p>to add multiline code is like this</p> <pre><code>a = 10\nfor i in range(a):\n    print(i)\n</code></pre> <p>and let's see mermaid instead</p> <pre><code>graph TD\n    A[Edit README.md] --&gt; B(Add Mermaid Code Block)\n    B --&gt; C{Commit Changes?}\n    C --&gt;|Yes| D[View Rendered Diagram on GitHub]\n    C --&gt;|No| E[Diagram won't show up]</code></pre>"},{"location":"guides/hydra/","title":"Hydra","text":"<p>It is a cool piece of software to manage configurations, afaik basically you can have a configuration tree and with the hydra decorator on the entry point it gets assembled and passed to the main. I closely relates to OmegaConf, which is like the format/python package to manage it.</p>"},{"location":"guides/screen/","title":"Screen","text":"<p>Is another terminal multiplexer, basically the father of tmux, much simpler but it comes preinstalled everywhere so that is pretty handy and it makes worth to know how to use it. The main point is to have multiple sessions on a server, so basically ssh then screen to open up a session, start a program (eg an ML training run) and then detach, in this way the run keeps going in the background and we can run other commands in our session without the need of multiple ssh sessions to the same server.</p>"},{"location":"guides/screen/#commands","title":"Commands","text":"<p>As usual C means Ctrl</p> <ul> <li><code>screen -ls</code> list of the current session with details</li> <li><code>screen -S &lt;session-name&gt;</code> creates new session and attaches to it</li> <li><code>C-d</code> detaches from session and kills it</li> <li><code>C-a-d</code> detaches from session without killing it</li> <li><code>screen -r &lt;session-name&gt;</code> attaches to running session</li> </ul>"},{"location":"guides/shells/","title":"Shells","text":"<p>What are the differences between various shells? what should one use?</p> <ul> <li>bash</li> <li>zsh</li> </ul>"},{"location":"guides/ssh/","title":"SSH and friends","text":"<p>All about remote shit, agent forwarding, git things etc</p>"},{"location":"guides/ssh/#sshfs","title":"SSHFS","text":"<p>I don't even know if this is actual ssh, but it is used to mount folders</p> <ul> <li><code>sshfs user@remote-server-ip:/path/to/remote/folder ~/local-folder</code> mounts the remote folder in the local folder</li> </ul>"},{"location":"guides/tmux/","title":"Tmux","text":"<p>tmux is a terminal multiplexer, that allows for having multiple terminal sessions, with different, sessions, windows and panes. It makes quite a bit of sense to use it especially if one has different sessions via ssh, and also wants to split into panes to have more flexibilty when using CLI stuff. There are no many reasons to not have it and it is so customizable and usable anywhere that it is kinda a super terminal learn it once use it everywhere.</p> <ul> <li><code>C</code> means Ctrl</li> <li><code>C-b</code> is the prefix for everything</li> <li><code>C-b ?</code> is the manual</li> <li><code>C-b %</code> splits vertically</li> <li><code>C-b arrows</code> to navigate the panes</li> </ul> <p>There are commands and keybindings, that makes sense, the command prompt is activated with <code>C-b :</code> and then one can type any command. some commands already have keybindings, other keybindings can be assigned I guess.</p>"},{"location":"guides/tmux/#sessions-windows-and-panes","title":"Sessions, windows and panes","text":"<p>A Session is the big container, with inside windows, and inside those panes, when you split the screen you are creating new panes.</p> <ul> <li><code>tmux a</code> to attach to a session from outside, the only session or the last one</li> <li><code>tmux a -t &lt;session_name&gt;</code> to a ttach to a specific session</li> <li><code>exit</code> to exit and kill the session</li> <li><code>C-b d</code> to detach from the session and let it run</li> </ul>"},{"location":"guides/tmux/#pane-commands","title":"Pane commands","text":"<ul> <li><code>split-window</code> is the main command to make new panes, main options are <code>-v</code> and <code>-h</code></li> <li><code>C-b %</code> splits vertically</li> <li><code>C-b \"</code> splits horizontally</li> <li><code>C-b arrows</code> to navigate the panes</li> <li><code>C-b x</code> closes the current pane, I guess killing anything inside it</li> </ul>"},{"location":"guides/tmux/#windows-commands","title":"Windows commands","text":"<ul> <li><code>:select-window</code> is the basic command to change window, then there are mulitple premade keybindings</li> <li><code>C-b c</code> new window, default name</li> <li><code>C-b ,</code> rename window</li> <li><code>C-b n</code> go to next window</li> <li><code>C-b p</code> go to previous window</li> <li><code>C-b 0-9</code> go to window n</li> <li><code>C-b w</code> list windows ` file, from apperance to keybindings. that's quite fun and you can make tmux quite personalized to your needs.</li> </ul>"},{"location":"guides/tmux/#mouse-mode","title":"Mouse mode","text":"<p>I activated the mous mode, it lets you switch panes by clicking and select things in the terminal but it is  a bit clunky. It needs some practice. Mouse mode can be activated adding <code>set -g mouse on</code> to <code>.tmux.conf</code></p>"},{"location":"guides/tmux/#customizations","title":"Customizations","text":"<p>Some stuff can be customized in the <code>.tmux.conf</code> file, from apperance to keybindings. that's quite fun and you can make tmux quite personalized to your needs.</p>"},{"location":"guides/uv/","title":"UV","text":"<p>I don't really know it's like pip but better in some way.</p>"},{"location":"guides/uv/#commands","title":"Commands","text":"<ul> <li><code>uv pip install &lt;package-name&gt;</code> install a python package</li> </ul>"},{"location":"guides/vim/","title":"VIM Guide","text":""},{"location":"guides/vim/#useful-commands","title":"useful commands","text":"<ul> <li><code>:Ex</code> opens pwd</li> <li><code>%</code> new file from the explorer</li> <li> <p><code>d</code> new directory from the explorer</p> </li> <li> <p><code>:bd</code> closes current buffer</p> </li> <li><code>:w</code> saves the file</li> </ul>"},{"location":"machine-learning/","title":"Machine Learning","text":"<p>Machine learning concepts, algorithms, and implementations.</p>"},{"location":"machine-learning/ml-notes/","title":"ML-random notes","text":"<p>I write stuff here, ideally they are to be moved once they make more sense in the bigger context</p>"},{"location":"machine-learning/ml-notes/#ctc-loss","title":"CTC Loss","text":"<p>Connectionist Temporal Classification it is a loss used for RNNs, seems to be the go to for phonemes classification. Basically ignores repetition and blanks in sequences, since in audio there are much more frames than phonemes.</p> <p><code>C C A - - T -</code> \u2192 <code>C A T</code> </p>"},{"location":"machine-learning/ml-notes/#multinomial-sampling","title":"Multinomial sampling","text":"<p>From logits we can do probabilities (softmax), at that point we can sample from thisdistribution with multinomial sampling instead of argmax. This from what I understand works well but it is not differentiable so it is to be used with care</p>"},{"location":"machine-learning/ml-notes/#gumbel-softmax","title":"Gumbel softmax","text":"<p>We can do a crazy trick that makes sampling differentiable, basically we to the apply some noise and then we do the softmax, and this is like sampling but differentiable. which kinda makes sense: <code>softmax{1 + gumbel(0,1)}</code></p> <p>The softmax is soft and gumbel is similar to gaussian noise, with the forward pass we get the index, with the backward pass we get the gradient, this makes it fully differentiable and very important to properly explore the gradient</p> <p>Interesting video to dig deeper: https://www.youtube.com/watch?v=c65nlNgfFrE</p>"},{"location":"machine-learning/ml-notes/#autocorrelation-issues","title":"Autocorrelation Issues","text":"<p>The data has similar patterns in different segments. That's obvious, especially for audio but it is good to be clear.</p>"},{"location":"machine-learning/ml-notes/#span-masking","title":"Span masking","text":"<p>Since in audio there is a lot of autocorrelation if we just mask a frame it's very easi to predict, just copy and paste the previous one. That works most of the time but it is clearly useless, to solve this what is done it is called span masking, which means that we mask subsequent frames all together, so that actual changes and information is masked, if a model is successful on this it means that it is learning quite a lot about how to fill in the blanks.</p>"},{"location":"machine-learning/ml-notes/#contrastive-learning","title":"Contrastive learning","text":"<p>Contrastive learning is basically a loss function, it works like this: we have a (or more than one) positive sample (vector), the one that our input should match and a few negative ones, that our input should not match, and the loss function is minimizing the distance with the positive and maximizing the distance with the negatives, so in summary:</p> <ul> <li>One or more positive sample (targets)</li> <li>Many negative samples (non-targets)</li> <li>loss function minimizes distance with positives and maximizes with negatives</li> </ul> <p>There are some tricky parts, in particular chosing hard enough negatives without exaggerating, and feeding good poisitives, that are often augmented so that the shape changes but not the semantics.</p>"},{"location":"machine-learning/ml-notes/#diversity-loss","title":"Diversity loss","text":"<p>Often when quantizing we can experience index collapse, everything is mapped to the same thing, the loss is zero but everything is useless. so we use a diversity loss. Basically we take the distribuition of the notebooks, which is like a probability distribution, and we want it to have max entropy, thus a univorm distribution and we use as a loss the difference between our distribution and a uniform one. That is basically entropy, we want max entropy and so we just measure our entropy and loss is based on that. We just want the probability of each entry to be \\(1/K\\).</p>"},{"location":"machine-learning/maths/","title":"Mathematics","text":"<p>Mathematical foundations for machine learning.</p>"},{"location":"machine-learning/maths/gaussianmixturemodel/","title":"Gaussian Mixture Model","text":"<p>It is kinda similar to k-means, but instead of hard clusters it finds gaussians in the points. it is part of the mixture models, that I guess given a population decompose it as a sum of subpopulations, thus distributions.</p>"},{"location":"machine-learning/maths/kmeans/","title":"K-means","text":"<p>It is a method to cluster data, which is de-facto quantization, and divides the space into voronoi cells, starting from random displaced centroids moving them around until they're good. It is a discrete latent variable model.</p>"},{"location":"machine-learning/maths/kmeans/#algorithm","title":"Algorithm","text":"<pre><code>def kmeans(k, points):\n    centroids = init_random_points(k)\n    converged = False\n\n    while not converged:\n        clusters = [[] for _ in range(k)]\n\n        for point in points:\n            cluster = argmin([distance(point, centroid) for centroid in centroids])\n            clusters[cluster].append(point)\n\n        new_centroids = [mean(cluster) for cluster in clusters]\n\n        if new_centroids == centroids:\n            converged = True\n        else:\n            centroids = new_centroids\n\n    return centroids, clusters\n</code></pre>"},{"location":"machine-learning/maths/probability/","title":"Probability","text":""},{"location":"machine-learning/maths/probability/#probability-distribuition","title":"Probability distribuition","text":"<p>From what a probability distribution is just a function that maps the possible outcome to its probability. It makes sense that the total probability (the integral I guess) is 1. In the case of a continuous domain it feels a bit weird, I guess that in that case you always integrate and the probability is actually the area(?). </p>"},{"location":"machine-learning/maths/probability/#uniform-distribution","title":"Uniform distribution","text":"<p>Well this is just flat, every result is equally probable.</p>"},{"location":"machine-learning/maths/probability/#entropy","title":"Entropy","text":""},{"location":"machine-learning/maths/probability/#perplexity","title":"Perplexity","text":""},{"location":"machine-learning/maths/softmax/","title":"Softmax","text":""},{"location":"papers/","title":"Papers","text":"<p>Notes on research papers and academic literature.</p>"},{"location":"papers/hubert/","title":"HuBERT","text":""},{"location":"papers/hubert/#concept","title":"Concept","text":"<p>It is predictive not contrastive, which is quite abig difference, it doesn't pick the right one ouf of x like wav2vec but it fills in the blank just with entropy loss, the targets are cluster index of the input audio. </p> <pre><code>flowchart LR\n    S[audio] --&gt; C(CNN)\n    S --&gt; D(clustering)\n    C --&gt; |masking|B(transformer)\n    D &lt;--&gt; |cross entropy| B\n    T --&gt; |refining| D\n    B --&gt; |final| T[rich tensor]</code></pre> <p>Once it is trained the input is just audio and the final output is a semantically and context rich representation than can be used for various tasks like, transcription, emotion detection, or speaker identification</p>"},{"location":"papers/hubert/#architecture","title":"Architecture","text":"<p>The architecture is the standard wav2vec architecture:</p> <pre><code>flowchart LR\n    A(CNN 1d 7 blocks) --&gt; p1(Linear 512-&gt;768)\n    p1 --&gt; B(BERT, 12+ layers)\n    B --&gt; p2(Linear -&gt; codebook)</code></pre>"},{"location":"papers/hubert/#cnn","title":"CNN","text":"<p>The CNN is 7, blocks, 1d, the first applies a downsampling of 10x, the following of 2x, for a total downsampling of 320x, getting from 16khz to 50hz rate. All the layers have 512 channels.</p>"},{"location":"papers/hubert/#bert","title":"BERT","text":"<p>BERT stands for Bidirectional Encoder Representations from Transformers. the idea is that it is made to build a meaningful and contextful representation</p>"},{"location":"papers/hubert/#projection","title":"Projection","text":""},{"location":"papers/hubert/#training","title":"Training","text":""},{"location":"papers/hubert/#hidden-audio-units","title":"Hidden audio units","text":"<p>So we have an utterance (vocal sound), goes through the CNN and it is now encoded, \\(X=[x_0, x_1, ... x_T]\\) of \\(T\\) frames. We take a clustering model (eg kmeans) and we have now hidden units (ie labels, cluster index, discrete stuff): \\(h(X) = Z = [z_0, z_1, ..., z_T]\\) we call this hidden because it is like internal of the machine, we don't impose the clusters and we don't really care what they are.</p>"},{"location":"papers/hubert/#masking","title":"Masking","text":"<p>On this audio units we apply a mask at the indices \\(M \\subset [T]\\) and we obtain \\(\\tilde{X}\\) a version of \\(X\\) in which some frames are replaced with the masking embedding \\(\\tilde{x}\\). This is applied in a span masking fashion with a masking factor of around 50%, that means that long contiuous portions will be masked.</p>"},{"location":"papers/hubert/#loss","title":"Loss","text":"<p>Cross-Entropy loss (right class = good) is used to train the model, and it is only calculated on the masked units, in this way the model can actually learn to understand the context and doesn't just mimick the clustering model</p>"},{"location":"papers/hubert/#refining-the-clustering","title":"Refining the clustering","text":"<p>Several tricks are used to get a better clustering:</p> <ul> <li> <p>cluster ensambles: multiple k means models are used in parallel, with different Ks thus different granularity</p> </li> <li> <p>latent clustering: new generations of clusters are done directly on the learn latents, not on the audio MFCCs, and with the proceeding of the learning this clusters become more and more meaningful</p> </li> </ul>"},{"location":"papers/hubert/#fine-tuning","title":"Fine tuning","text":"<p>Like for wav2vec a final supervised fine tuning is applied with CTC loss with the CNN frozen</p>"},{"location":"papers/hubert/#result","title":"Result","text":""},{"location":"papers/hubert/#take-aways","title":"Take-aways","text":""},{"location":"papers/sidon/","title":"Sidon","text":"<p>https://arxiv.org/pdf/2509.17052</p> <p>The idea is having a super smart encoder (a transformer, like HuBERT or similar) for a DAC-like latent space, and adapt the decoder to decode from this new encoder</p> <pre><code>flowchart LR\n    A[feature predictor] --&gt; L{latent} --&gt; V[Vocoder]</code></pre>"},{"location":"papers/sidon/#architecture","title":"Architecture","text":"<p>feature predictor is w2v-BERT, with a linear layer (using lora) added in each block</p> <p>Vocoder is HiFi-GAN, same concept as descript audio decoder</p>"},{"location":"papers/sidon/#training","title":"Training","text":"<ul> <li> <p>Feature predictor is trained, mapping noisy audio to clean feature (MSE between features)</p> </li> <li> <p>Vocoder is trained, from clean feature to clean audio (MSE between Mel spectrograms + Feature Matching Loss)</p> </li> <li> <p>Vocoder is fine-tuned, from predicted feature to clean audio (MSE between Mel spectrograms + Feature Matching Loss)</p> </li> </ul>"},{"location":"papers/todo-reads/","title":"Papers to read","text":""},{"location":"papers/todo-reads/#speech-repreentation","title":"Speech repreentation","text":"<ul> <li>[x]  wav2vec 2.0</li> <li>[x]  HuBERT</li> <li>[ ]  WavLM</li> <li>[ ]  w2v BERT https://arxiv.org/pdf/2108.06209</li> </ul>"},{"location":"papers/todo-reads/#audio-stuff","title":"Audio stuff","text":"<ul> <li>[ ]  EnCodec</li> <li>[ ]  HiFI-GAN https://arxiv.org/pdf/2010.05646</li> </ul>"},{"location":"papers/todo-reads/#denoisers","title":"Denoisers","text":"<ul> <li>[x]  sidon https://arxiv.org/pdf/2509.17052</li> </ul>"},{"location":"papers/todo-reads/#machine-learning","title":"Machine Learning","text":"<ul> <li>[ ]  BERT https://arxiv.org/pdf/1810.04805</li> </ul>"},{"location":"papers/wav2vec2/","title":"wav2vec 2.0","text":"<p>Basically audio in, quantized tokens out</p>"},{"location":"papers/wav2vec2/#architecture","title":"Architecture","text":"<p>There's a CNN and then a transformer</p>"},{"location":"papers/wav2vec2/#quantizer","title":"Quantizer","text":"<p>it works with codebooks, there are two codebooks, each of them 320 long, a vector is chosen from each codebook and then they are conccatenated (adn then a linear projection but whatever)</p>"},{"location":"papers/wav2vec2/#training","title":"Training","text":"<p>Everything is trained together:</p> <ul> <li>CNN takes audio (20ms) and outputs non-quantized vector(z)</li> <li>z get quantized with a quantizer module (q)</li> <li>z gets relative positional encoding with a small CNN (? i think it's here)</li> <li>z get masked (49%) and go into the transformer</li> <li>transformer is trained to guess the quantized missing token</li> <li>Loss 1: contrastive so positive target and distractors</li> <li>Loss 2: diversity, entropy of the probability distribution of the codebooks</li> <li>fine tuning step with labeled data and ctc loss</li> </ul>"},{"location":"papers/wav2vec2/#result","title":"Result","text":"<p>We throw away the quantizer and we just use the CNN and transformer, this leads to get context aware tokens and works very well </p>"},{"location":"python/","title":"Python","text":"<p>Python programming notes and guides.</p>"},{"location":"python/debugger.py/","title":"Python Debugger","text":"<p><code>pdb</code></p>"},{"location":"python/path/","title":"pathlib","text":"<p>Let's try to understand how this works, I feel like it is quite relevant if we want to deal with files.</p> <p>Before there was os to deal with this kind of stuff, and it treated paths as strings. Now we can use Path and treat them as objects. In pathlib it's a tiny bit slower, but it manages platforms and a lot of other things, way more powerful an basically the go to, at least since python 3.12</p> <p>Let's start by creating a path and using the join <code>/</code> operator</p> <pre><code>from pathlib import Path\n\nbase_dir = Path('/home/tito')\npython_dir = base_dir / 'test'\nthis_file = python_dir / 'path.py'\n\nprint(base_dir)\nprint(python_dir)\nprint(this_file)\n</code></pre> <p>prints: <code>/home/tito</code> prints: <code>/home/tito/test</code> prints: <code>/home/tito/test/path.py</code> Now let's check some utils:</p> <ul> <li><code>name</code> returns the filename</li> <li><code>stem</code> returns filename without extension</li> <li><code>suffix</code> returns file extension</li> <li><code>parent</code> returns the directory containing the file</li> </ul> <pre><code>print(this_file.name)\nprint(this_file.stem)\nprint(this_file.suffix)\nprint(this_file.parent)\n</code></pre> <p>prints: <code>path.py</code> prints: <code>path</code> prints: <code>.py</code> prints: <code>home\\tito\\test</code>**n **</p>"},{"location":"python/unpack-operators/","title":"Unpack operators","text":"<p>Runnable gist:https://gist.github.com/inspektral/14763622f1d273791ded8afce78e90d5</p> <pre><code>def sum(*args):\n    sum = 0\n    for x in args:\n        sum += x\n    return sum\n\nprint(sum(1,2,3))\n</code></pre> <p>prints <code>6</code>, and this is all fine, but how do those * work?</p> <pre><code>a = [1,2,3]\nprint(a, *a)\n</code></pre> <p>prints <code>[1, 2, 3] 1 2 3</code></p> <p>So they work in both ways? How? What? they are called unpack operators, and fine in front of iterables they can unpack them, so things like this are possible:</p> <pre><code>l1 = [1,2,3,4]\nl2 = [5,6,7,8]\n\nl3 = [*l1, *l2]\n\nprint(l1, l2, l3)\n</code></pre> <p>Prints: <code>[1, 2, 3, 4] [5, 6, 7, 8] [1, 2, 3, 4, 5, 6, 7, 8]</code>and now the weird part arrives...</p> <pre><code>a, *b, c = l3\nprint(a,b,c)\n</code></pre> <p>Prints: <code>1 [2, 3, 4, 5, 6, 7] 8</code> so here is like a pack operator? </p> <p>Apparently there are various operations that the CPython interpeter can to UNPACK_EX, when the * is on the left of an assignment like:</p> <pre><code>a, *b, c = [1,2,3,4]\n</code></pre> <p>It looks onto the right of the assignement, checks how much stuff there is, assigns it to the non-* variables and the rest to the * variables as a LIST</p> <pre><code>import dis\n\ndef test_assignment():\n    l = [1, 2, 3]\n    a, *b = l\n\ndis.dis(test_assignment)\n</code></pre> <p>the *args thing, which is happens before the function call, completely in C, that assigns the arguments to a TUPLE</p> <pre><code>def test_func(*args):\n    return args\n\ndis.dis(test_func)\n</code></pre> <p>In general, besides the usual differences between tuple and list they are, very similar. It is done like this because the unpack is usually done to manipulate the slice, and to be consistent it is a slice, which is much heavier, on the contrary the *args are usually not modified in place, so a tuple, much more efficient is used</p>"}]}